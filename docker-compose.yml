version: "3.9"

services:

  de_mysql:
    image: mariadb:10.11.2
    container_name: de_mysql
    # volumes:
    #   - ./mysql:/var/lib/mysql
    ports:
      - "3306:3306"
    env_file:
      - .env
    networks:
      - data_network

  de_psql:
    image: postgres:15
    container_name: de_psql
    # volumes:
    #   - ./psql:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    env_file:
      - .env
    networks:
      - data_network

  minio:
    hostname: minio
    image: "minio/minio"
    container_name: minio
    ports:
      - "9001:9001"
      - "9000:9000"
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - ./minio/data:/data
    env_file:
      - .env
    networks:
      - data_network

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    image: "bitsondatadev/hive-metastore"
    entrypoint: /entrypoint.sh
    ports:
      - "9083:9083"
    volumes:
      # - ./hive-metastore/entrypoint.sh:/entrypoint.sh
      - ./hive-metastore/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
    environment:
      METASTORE_DB_HOSTNAME: de_mysql
    networks:
      - data_network
    depends_on:
      - de_mysql
      - minio

  spark-master:
    build:
      context: ./spark
      dockerfile: ./Dockerfile
    container_name: "spark-master"
    ports:
      - "7077:7077"  # Spark master port
      - "8081:8080"  # Spark master web UI port
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    volumes:
      - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    networks:
      - data_network

  spark-worker-1:
    image: docker.io/bitnami/spark:3.3
    container_name: "spark-worker-1"
    env_file:
      - .env
    depends_on:
      - spark-master
    networks:
      - data_network

  spark-worker-2:
    image: docker.io/bitnami/spark:3.3
    container_name: "spark-worker-2"
    env_file:
      - .env
    depends_on:
      - spark-master
    networks:
      - data_network

  spark-thrift-server:
    build:
      context: ./spark
      dockerfile: ./Dockerfile
    container_name: "spark-thrift-server"
    restart: always
    depends_on:
      - spark-master
      - hive-metastore
    ports:
      - "4040:4040"
      - "10000:10000"
    command: sh -c "
      sleep 10 && ./sbin/start-thriftserver.sh --driver-java-options '-Dhive.metastore.uris=thrift://hive-metastore:9083' --master spark://spark-master:7077      --executor-memory 4G --total-executor-cores 4 --driver-memory 4G"
    volumes:
      - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./spark/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
    networks:
      - data_network

  metabase:
    image: metabase/metabase:latest
    container_name: "metabase"
    ports:
      - "3000:3000"
    env_file:
      - .env
    networks:
      - data_network

  airflow-webserver: 
    build: ./airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      - mysql-airflow
    environment:
      - LOAD_EX=n
      - AIRFLOW__CORE__FERNET_KEY=airflow_fernet_key
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflow_secret_key
      - AIRFLOW__WEBSERVER__SECRET_KEY_CMD=airflow_secret_key
      - AIRFLOW__WEBSERVER__SECRET_KEY_SECRET=airflow_secret_key
      - AIRFLOW__WEBSERVER__RBAC=False
      - AIRFLOW__WEBSERVER__SESSION_BACKEND=securecookie
      - AIRFLOW_CORE_MYSQL_CONN=mysql+mysqlconnector://airflow:airflow@mysql-airflow:3307/airflow
      - EXECUTOR=Local    
    volumes:
        - ./restaurant_analytis:/usr/local/airflow/restaurant_analytis
        - ./mlflow/project:/usr/local/airflow/mlflow-project
        - ./airflow/dags:/usr/local/airflow/dags
        - ./airflow/logs:/usr/local/airflow/logs
        - ./airflow/plugins:/usr/local/airflow/plugins
        - ./airflow/config/airflow.cfg:/usr/local/airflow/config/airflow.cfg
        - ./airflow/jars:/usr/local/airflow/jars
        - ./scripts:/usr/local/airflow/scripts
    ports:
        - "8088:8088"
        - "8888:8888"
    command: webserver
    healthcheck:
        test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
        interval: 30s
        timeout: 30s
        retries: 3
    networks:
        - data_network

  mysql-airflow:
    image: mysql:5.7
    container_name: mysql-airflow
    ports:
      - "3307:3307"
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=airflow
      - MYSQL_USER=airflow
      - MYSQL_PASSWORD=airflow
      - MYSQL_TCP_PORT=3307
    expose:
      - "3307"
    command: --default-authentication-plugin=mysql_native_password
    volumes:
      - ./mysql/my.cnf:/etc/mysql/my.cnf
    networks:
      - data_network

  mlflow:
    build:
      context: ./mlflow
      dockerfile: Dockerfile
    container_name: mlflow
    ports:
      - "5000:5000"
    environment:
      - PORT=5000
      - FILE_DIR=/mlflow
      - AWS_BUCKET=mlflow
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    networks:
      - data_network

  model_serving:
    build:
      context: ./model_server
      dockerfile: Dockerfile
    container_name: recommender_model
    ports:
      - "5001:5001"
    volumes:
      - ./model_server/main.py:/opt/mlflow/main.py
    environment:
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - AWS_BUCKET=mlflow
      - FILE_DIR=/mlflow
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    networks:
      - data_network

  app:
    build:
      context: ./app
      dockerfile: Dockerfile
    container_name: recommender_app
    ports:
      - "8051:8051"
    command: bash -c "export PYTHONPATH=/home/app"
    volumes:
      - ./app:/home/app
    networks:
      - data_network
      

networks:
  data_network:
    driver: bridge
    name: data_network
