{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.csv(\"a.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data, test_data = df.randomSplit([0.7, 0.2,0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StreamingRecommendationModel' from 'pyspark.ml.recommendation' (D:\\spark\\python\\pyspark\\ml\\recommendation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstreaming\u001b[39;00m \u001b[39mimport\u001b[39;00m StreamingContext\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecommendation\u001b[39;00m \u001b[39mimport\u001b[39;00m StreamingRecommendationModel\n\u001b[0;32m      5\u001b[0m \u001b[39m# Khởi tạo SparkContext và StreamingContext\u001b[39;00m\n\u001b[0;32m      7\u001b[0m rank \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'StreamingRecommendationModel' from 'pyspark.ml.recommendation' (D:\\spark\\python\\pyspark\\ml\\recommendation.py)"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml.recommendation import StreamingRecommendationModel\n",
    "\n",
    "# Khởi tạo SparkContext và StreamingContext\n",
    "\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(train_data, rank, numIterations)\n",
    "\n",
    "# Tạo model streamingALS từ model ALS đã xây dựng\n",
    "streaming_model = StreamingRecommendationModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đưa ra dự đoán cho các sự kiện mới\n",
    "stream_items = stream_ratings.map(lambda rating: rating[1]).distinct()\n",
    "stream_users = stream_ratings.map(lambda rating: rating[0]).distinct()\n",
    "stream_new_ratings = stream_users.cartesian(stream_items).map(lambda x: (x[0], x[1]))\n",
    "stream_predictions = streaming_model.predictOn(stream_new_ratings).map(lambda rating: ((rating[0], rating[1]), rating[2]))\n",
    "\n",
    "# In ra kết quả dự đoán\n",
    "stream_predictions.pprint()\n",
    "\n",
    "# Khởi chạy StreamingContext\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(rank=10, maxIter=10, regParam=0.01, userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\")\n",
    "\n",
    "# Khởi tạo dataframe với schema (user_id, item_id, rating)\n",
    "df = spark.createDataFrame([(1, 1, 5.0), (1, 2, 3.0), (2, 1, 4.0)], [\"user_id\", \"item_id\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\spark\\python\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(df)\n",
    "U_new = model.userFactors\n",
    "V_new = model.itemFactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def convergence_criteria(U, V, U_new, V_new, tol=1e-3):\n",
    "    # Tính độ tương đồng giữa hai ma trận factorization liên tiếp\n",
    "    U_new_arr = U_new.toPandas().values\n",
    "    similarity = np.abs(np.dot(U_new_arr.T, V_new) - np.dot(U.toPandas().values.T, V))\n",
    "    \n",
    "    # Kiểm tra xem độ tương đồng có đạt mức đủ nhỏ hay không\n",
    "    if np.max(similarity) < tol:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_new_data():\n",
    "    # Lấy một tương tác người dùng ngẫu nhiên từ cơ sở dữ liệu tương tác\n",
    "    user_id = random.randint(1, 10)\n",
    "    item_id = random.randint(1, 10)\n",
    "    rating = random.uniform(1, 5)\n",
    "    my_df = spark.createDataFrame([(user_id, item_id, rating)], schema=[\"user_id\", \"item_id\",\"rating\"])\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random\n",
    "V = np.random\n",
    "while True:\n",
    "    # Lấy một mẫu dữ liệu đến mới\n",
    "    new_data = get_new_data()\n",
    "    \n",
    "    # Thêm dữ liệu đến mới vào dataframe\n",
    "    new_df = spark.createDataFrame([new_data], [\"user_id\", \"item_id\", \"rating\"])\n",
    "    df = df.union(new_df)\n",
    "    \n",
    "    # Tính toán ma trận factorization mới\n",
    "    model = als.fit(df)\n",
    "    U_new = model.userFactors\n",
    "    V_new = model.itemFactors\n",
    "    \n",
    "    # Kiểm tra tiêu chí dừng\n",
    "    if convergence_criteria(U, V, U_new, V_new):\n",
    "        break\n",
    "    \n",
    "    # Cập nhật ma trận factorization\n",
    "    U, V = U_new, V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for initial model: 1.7949632629184065\n",
      "RMSE for loop 1: 1.9976266316803237\n",
      "RMSE for loop 2: 1.7949632629184065\n",
      "RMSE for loop 3: 0.9819193816927092\n",
      "RMSE for loop 4: 2.269935398325997\n",
      "RMSE for loop 5: 1.7949632629184065\n",
      "RMSE for loop 6: 2.2865446382552346\n",
      "RMSE for loop 7: 1.7949632629184065\n",
      "RMSE for loop 8: 1.7949632629184065\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "tolerance = 0.001\n",
    "num_loops=10\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "# Khởi tạo mô hình ALS với các tham số\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\",nonnegative=True,implicitPrefs = False,\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "# Tạo mô hình ALS ban đầu bằng cách fit với dữ liệu đầu vào\n",
    "initial_model = als.fit(train_data)\n",
    "\n",
    "# Sử dụng mô hình ban đầu để đưa ra các dự đoán cho tập validation\n",
    "predictions = initial_model.transform(val_data)\n",
    "\n",
    "# Tính toán RMSE cho mô hình ban đầu\n",
    "try:\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "except:\n",
    "    rmse=0\n",
    "last_rmse=rmse\n",
    "print(\"RMSE for initial model: {}\".format(rmse))\n",
    "\n",
    "# Lặp lại quá trình huấn luyện với dữ liệu mới\n",
    "for i in range(num_loops):\n",
    "    # Lấy ra tập dữ liệu mới\n",
    "    new_data = get_new_data()\n",
    "    \n",
    "    # Sử dụng mô hình hiện tại để đưa ra dự đoán cho tập dữ liệu mới\n",
    "    new_predictions = initial_model.transform(new_data)\n",
    "    \n",
    "    # Kết hợp dữ liệu mới với dữ liệu cũ để tạo thành một tập dữ liệu lớn hơn\n",
    "    combined_data = train_data.union(new_data)\n",
    "    \n",
    "    # Tạo mô hình mới bằng cách fit với tập dữ liệu mới hơn\n",
    "    new_model = als.fit(combined_data)\n",
    "    \n",
    "    # Sử dụng mô hình mới để đưa ra dự đoán cho tập validation\n",
    "    new_predictions = new_model.transform(val_data)\n",
    "    \n",
    "    # Tính toán RMSE cho mô hình mới\n",
    "    try:\n",
    "        rmse = evaluator.evaluate(new_predictions)\n",
    "    except:\n",
    "        rmse = 0\n",
    "    print(\"RMSE for loop {}: {}\".format(i+1, rmse))\n",
    "    \n",
    "    # Kiểm tra xem sự cải thiện của mô hình có đạt mức đủ nhỏ hay không\n",
    "    if abs(rmse - last_rmse) < tolerance:\n",
    "        break\n",
    "    \n",
    "    # Cập nhật mô hình hiện tại và RMSE\n",
    "    initial_model = new_model\n",
    "    last_rmse = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
